{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Implement REINFORCE and use it to solve Gymansium CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate the policy with a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the RL environement\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a standard feed-forward neural network class\n",
    "def mlp(sizes: list, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = (\n",
    "            activation if j < len(sizes) - 2 else output_activation\n",
    "        )  # A cool declaration using if/else\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, sizes: list, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        self.NN = mlp(sizes, activation, output_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.NN(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NN that approximates the policy\n",
    "nn_policy = NeuralNetwork([env.observation_space.shape[0], 32, env.action_space.n])\n",
    "\n",
    "\n",
    "def policy(observation):\n",
    "    logits = nn_policy(torch.as_tensor(observation, dtype=torch.float32))\n",
    "    return Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - Untrained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 24.0 \n",
      "Episode 2: 36.0 \n",
      "Episode 3: 25.0 \n",
      "Episode 4: 15.0 \n",
      "Episode 5: 14.0 \n"
     ]
    }
   ],
   "source": [
    "# Untrained policy\n",
    "env_test = gym.make(\"CartPole-v1\", render_mode = 'human')\n",
    "episodes = 5\n",
    "episode_rewards = []\n",
    "for episode in range(1, episodes + 1):\n",
    "    observation, _ = env_test.reset()\n",
    "    score = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not terminated or truncated:\n",
    "        action = policy(observation).sample().item()\n",
    "        observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode {episode}: {score} \")\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the policy objective function\n",
    "\n",
    "Our policy objection function is the expected reward following the policy starting form the initial state. However, we note that from the policy gradient theorem, we can equivalently use as the objective function\n",
    "$$\n",
    "J = \\mathbb{E}_{\\pi_\\theta} \\left[ \\log \\pi_\\theta(s,a) Q^{\\pi_\\theta}(s,a) \\right] ~.\n",
    "$$\n",
    "We can approximate this by the actual return following the policy (Monte-Carlo sampling). I.e., we have\n",
    "$$\n",
    "J \\approx \\log \\pi_\\theta(s_t,a_t) G_t ~.\n",
    "$$\n",
    "We will simplify things further by setting $G_t = G_1$ for all steps in an episode. I.e., we have\n",
    "$$\n",
    "J \\approx \\log \\pi_\\theta(s_t,a_t) G_1 ~.\n",
    "$$\n",
    "For simplicity, we further set the discount factor to be 1. I.e., we don't discount energything, $G_0$ is the sum of rewards obtained throughout the epsiode. In Cartpole, this is the same as the episode length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 5000 steps and return batch observations, actions, and ep length. These will act as training data for our loss function.\n",
    "def collect_experience(batch_size=5000):\n",
    "    batch_observations = []\n",
    "    batch_actions = []\n",
    "    batch_weights = []\n",
    "\n",
    "    while True:\n",
    "        observation, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_length = 0\n",
    "\n",
    "        while True:\n",
    "            episode_length += 1\n",
    "            action = policy(observation).sample().item()\n",
    "            batch_actions.append(action)\n",
    "            batch_observations.append(observation)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        batch_weights += [episode_length] * episode_length\n",
    "        if len(batch_observations) > batch_size:\n",
    "            break\n",
    "\n",
    "    return np.array(batch_observations), batch_actions, batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (defined as - 1 times objection function)\n",
    "def compute_loss(obserations, actions, weights):\n",
    "    logp = policy(obserations).log_prob(actions) # This gives log \\pi(s_t, a_t). log_prob is a method of nn.Categorical.\n",
    "    return - (logp * weights).mean() # The - is because torch optimiser default optimisation direction is Desend and not Acsend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise policy objective function\n",
    "\n",
    "We shall not be using vanilla stochastic gradient descent but batch ADAM. I.e., we will collect around 5000 data points before doing 1 update instead of updating at every data point. The optimiser is ADAM instead of vanilla stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:38<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up an optimizer\n",
    "optimizer = torch.optim.Adam(params=nn_policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train with 300 batches of 5000 data points (300 policy updates with each informed by 5000 time steps)\n",
    "num_epoch = 300\n",
    "batch_size = 5000\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    observations, actions, weights = collect_experience()\n",
    "    loss = compute_loss(\n",
    "        torch.as_tensor(observations, dtype=torch.float32),\n",
    "        torch.as_tensor(actions, dtype=torch.float32),\n",
    "        torch.as_tensor(weights, dtype=torch.float32),\n",
    "    )\n",
    "    optimizer.zero_grad()  # Reset the parameters in the optimiser (which might have been saved in memory from the last epoch)\n",
    "    loss.backward()  # Compute partial dervatives of loss w.r.t to model parameters\n",
    "    optimizer.step()  # Perform the parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean reward is 499.196 and the standard deviation is 5.0093496583888015.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "total_rewards = []\n",
    "episodes = 1000\n",
    "for episode in tqdm(range(episodes)):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        # Perform the chosen action and observe the next state and reward\n",
    "        action = policy(state).sample().item()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        if terminated or truncated:\n",
    "            total_rewards.append(score)\n",
    "            break\n",
    "total_rewards = np.array(total_rewards)\n",
    "env.close()\n",
    "\n",
    "print(\n",
    "    f\"The mean reward is {np.mean(total_rewards)} and the standard deviation is {np.std(total_rewards)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
