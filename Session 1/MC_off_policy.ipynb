{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equal probability for selecting any action in any state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(nS,nA):\n",
    "    action_prob = [1/nA] * nA\n",
    "    return [action_prob for _ in range(nS)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Always select the action with the highest Q-value (predicted future reward) for the current state to exploit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q, nS, nA):\n",
    "    # initialize arbitrary policy\n",
    "    policy = np.zeros([nS,nA])\n",
    "    \n",
    "    # list of states in environment \n",
    "    S = [i for i in range(nS)]\n",
    "\n",
    "    # loop over states\n",
    "    for state in S:\n",
    "        if all([ q == 0 for q in Q[state]]) :\n",
    "            policy[state] = random_policy(1,nA)[0]\n",
    "        \n",
    "        else:\n",
    "            best_action = np.argmax(Q[state]) \n",
    "            policy[state] = np.eye(nA)[best_action].tolist()\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ε-greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the exploration rate ε\n",
    "- Generate a random number between 0 and 1.\n",
    "- If the generated random number is less than ε, select a random action to explore.\n",
    "- Otherwise, select the action with the highest Q-value for the current state to exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, nS, nA, epsilon):\n",
    "    # initialize arbitraty policy\n",
    "    policy = [[0 for _ in range(nA)] for _ in range (nS)]\n",
    "    \n",
    "    # list of states in environment \n",
    "    S = [i for i in range(nS)]\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "\n",
    "    # loop over states\n",
    "    for state in S:\n",
    "\n",
    "        # if all Q values in state are 0, agent picks random action\n",
    "        if all([ q == 0 for q in Q[state][:]]):\n",
    "            for action in range(nA):\n",
    "                policy[state][action] = 1/nA\n",
    "\n",
    "        else:\n",
    "            # position of maximum Q value in state\n",
    "            maxQ = np.argmax(Q[state])\n",
    "\n",
    "            # probability agent acts randomly\n",
    "            prob = [epsilon/nA for _ in range(nA)]\n",
    "\n",
    "            # probability agent acts greedily\n",
    "            prob[maxQ] = 1 - epsilon  + (epsilon/nA)\n",
    "\n",
    "            for action in A:\n",
    "                policy[state][action] = prob[action]\n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate policy performance\n",
    "----------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate average episode reward following a given policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_average_reward(env, nA, policy, num_eval_episodes = 100):\n",
    "    # store episode rewards \n",
    "    episode_rewards = []\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "    \n",
    "    # loop over number of episodes \n",
    "    for _ in range(num_eval_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "\n",
    "        # loop until episode ends\n",
    "        while True:\n",
    "            # pick action according to the current policy\n",
    "            action = random.choices(A, weights=[policy[state][i] for i in range(nA)], k=1)[0]\n",
    "\n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # update current state\n",
    "            state = next_state\n",
    "\n",
    "            # count cumulative episode reward:\n",
    "            episode_reward += reward\n",
    "        \n",
    "            # exit loop when episode is terminated or truncated\n",
    "            if terminated==True or truncated == True:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                break\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    reward_average = np.mean(episode_rewards)\n",
    "\n",
    "    return reward_average"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen Lake environment from gymnasium API\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\",render_mode=\"ansi\",is_slippery=False)\n",
    "\n",
    "# restart episode with agent in state 0\n",
    "next_state, info = env.reset()\n",
    "\n",
    "print(env.render())\n",
    "\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "print(f\"Size of state space: {nS} \\nSize of action space: {nA}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-policy MC control\n",
    "----------------------\n",
    "---------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-policy methods are where actions taken by the agent for data aquisition follows a behaviour policy, which is different from the policy evaluated for control, called the target policy. This allows for more exploration of the state-action space that may have otherwise been missed if following the target policy, and thus reduces the risk of learning a sub-optimial policy. \n",
    "\n",
    "In order to evaluate the target policy from average returns sampled under the behaviour policy, we need to weight each return by an importance sampling ratio, which is the likelihood of the return under the target policy relative to the behaviour policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importance sampling ratio:\")\n",
    "Image(filename='Pseudo-code/importance_sampling_ratio.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"on-policy MC control algorithm:\")\n",
    "Image(filename='Pseudo-code/off-policy_MC_control.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount factor [0,1]\n",
    "gamma = # ...\n",
    "# Exploration rate [0,1]\n",
    "epsilon = # ...\n",
    "# Number of episodes [1,infty]\n",
    "num_episodes = # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offpolicy_control(env, nS, nA, num_episodes, alpha, gamma, epsilon):\n",
    "\n",
    "    # initialize arbitary action-value function\n",
    "    Q = # ...\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "    \n",
    "    # list to store cumulative sum of weights, W, for a given state-action value\n",
    "    C = np.zeros([nS,nA])\n",
    "\n",
    "    # initialize epsilon-greedy policy\n",
    "    b_policy = # ....\n",
    "\n",
    "    average_reward = evaluate_average_reward(env, b_policy, num_eval_episodes = 100)\n",
    "    \n",
    "    # store number of policy iterations and average episode reward during training\n",
    "    num_policy_iter = [0]\n",
    "    average_rewards_list_t = [average_reward]\n",
    "    average_rewards_list_b = [average_reward]\n",
    "\n",
    "    # number of policy iterations\n",
    "    n = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        n+=1\n",
    "        # initialize initial state value\n",
    "        state = 0\n",
    "\n",
    "        # save states, actions and rewards for each episode\n",
    "        states = [0]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # loop until episode terminates or truncates \n",
    "        while True:\n",
    "            b_policy = epsilon_greedy_policy(Q, nS, nA, epsilon = 0.1)\n",
    "            \n",
    "            # pick action according to the current behaviour policy\n",
    "            action = # ...\n",
    "\n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if terminated == False and truncated == False:\n",
    "                # save state, action and reward in episode\n",
    "                \n",
    "                # update current state\n",
    "                state = next_state\n",
    "\n",
    "            # if epsiode terminates or truncates, reset episode and exit loop\n",
    "            if terminated == True or truncated == True:\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # reset episode\n",
    "                # exit loop\n",
    "    \n",
    "        # initialize return value\n",
    "        G = 0\n",
    "\n",
    "        # initialize importance sampling ratio\n",
    "        W = 1\n",
    "\n",
    "        episode_length = len(states)\n",
    "        \n",
    "        # evaluate return for each state in episode\n",
    "        # loop backwards from the end of the episode, as return is a function of all future rewards in an episode\n",
    "        for i in reversed(range(episode_length)):\n",
    "\n",
    "            # return at the current state-action pair is equal to the sum of the immediate reward plus the future discounted return\n",
    "            G = # ...\n",
    "            \n",
    "            # cumulative sum of weights for each state-action pair\n",
    "            C[states[i],actions[i]] = C[states[i],actions[i]] + W\n",
    "\n",
    "            # incremental update rule for Q\n",
    "            Q[states[i],actions[i]]= # ...\n",
    "\n",
    "            t_policy = greedy_policy(Q,nS,nA)\n",
    "\n",
    "            # if action taken is not the optimal action under the target policy\n",
    "            if actions[i] != np.argmax(t_policy[states[i]]):\n",
    "                # then t_policy(s,a) = 0 (as target policy is deterministic) and so update of W = 0\n",
    "                break\n",
    "            \n",
    "            # if action taken is the optimial action, update W\n",
    "            else:\n",
    "                # where the probability the optimial action is taken under target policy is 1\n",
    "                W = # ...\n",
    "\n",
    "        average_reward_t = evaluate_average_reward(env, t_policy, num_eval_episodes = 100)\n",
    "        average_reward_b = evaluate_average_reward(env, b_policy, num_eval_episodes = 100)\n",
    "\n",
    "        average_rewards_list_t.append(average_reward_t)\n",
    "        average_rewards_list_b.append(average_reward_b)\n",
    "\n",
    "        num_policy_iter.append(n)\n",
    "    \n",
    "    return Q, t_policy, b_policy, num_policy_iter, average_rewards_list_t, average_rewards_list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, t_policy, b_policy, num_policy_iter, average_rewards_list_t, average_rewards_list_b = offpolicy_control(env, nS, nA, num_episodes, gamma, epsilon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the results\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rate\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(num_policy_iter, average_rewards_list_b, color = 'black', label = \"behaviour policy\")\n",
    "ax.plot(num_policy_iter, average_rewards_list_t, color = 'red', linestyle = 'dashed', label = \"target policy\")\n",
    "\n",
    "ax.set_xlabel(\"Number of policy iterations\")\n",
    "ax.set_ylabel('Average reward')\n",
    "\n",
    "ax.legend(prop = { \"size\": 10 })\n",
    "\n",
    "plt.xticks(np.arange(num_policy_iter[0], num_policy_iter[-1]+1,  num_policy_iter[-1]*0.1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
