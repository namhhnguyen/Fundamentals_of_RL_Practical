{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On-policy MC control\n",
    "----------------------\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onpolicy_control(env, nS, nA, num_episodes, gamma, epsilon):\n",
    "    # initialize action-value function\n",
    "    Q = np.zeros([nS,nA])\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "\n",
    "    # initialize list to save state-action returns for each epsisode\n",
    "    returns = [[[] for _ in range(nA)] for _ in range(nS)]\n",
    "\n",
    "    # initialize random policy\n",
    "    policy = random_policy(nS,nA)\n",
    "\n",
    "    average_reward = evaluate_average_reward(env, nA, policy, num_eval_episodes = 100)\n",
    "    \n",
    "    # store number of policy iterations and average episode reward during training\n",
    "    num_policy_iter = [0]\n",
    "    average_rewards_lists = [average_reward]\n",
    "\n",
    "    # number of policy iterations\n",
    "    n = 0\n",
    "\n",
    "    # loop over number of episodes of experience sampled by agent \n",
    "    for _ in range(num_episodes):\n",
    "        n+=1\n",
    "        # initialize initial state value\n",
    "        state = 0 \n",
    "        \n",
    "        # save states, actions and rewards for each episode\n",
    "        states = [0]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # loop until episode terminates or truncates \n",
    "        while True:\n",
    "            # epislon greedy policy\n",
    "            policy = epsilon_greedy_policy(Q, nS, nA, epsilon = 0.1)\n",
    "\n",
    "            # pick action according to the current policy\n",
    "            action = random.choices(A, weights=[policy[state][i] for i in range(nA)], k=1)[0]\n",
    "\n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            if terminated == False and truncated == False:\n",
    "                # save state, action and reward in episode\n",
    "                states.append(next_state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "\n",
    "                # update current state\n",
    "                state = next_state\n",
    "\n",
    "            # if epsiode terminates or truncates, reset episode and exit loop\n",
    "            if terminated == True or truncated == True:\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                env.reset()\n",
    "\n",
    "                break\n",
    "        \n",
    "        # initialize return value: the total discounted award reward from a complete episode\n",
    "        G = 0\n",
    "\n",
    "        episode_length = len(states)\n",
    "\n",
    "        state_action = [[states[i],actions[i]] for i in range(episode_length)]\n",
    "        \n",
    "        # evaluate G for each state in episode\n",
    "        # as return is a function of all future rewards of an episode,loop backwards in time\n",
    "        for i in reversed(range(episode_length)):\n",
    "            # return at the current state-action pair is equal to the sum of the immediate reward plus the future discounted return\n",
    "            G = gamma*G + rewards[i]\n",
    "\n",
    "            # first-visit MC averages averages over returns from first state-action pair in an episode\n",
    "            # ignore returns from state-action pairs after first time in epsiode\n",
    "            if state_action[i] not in state_action[:i]:\n",
    "                returns[states[i]][actions[i]].append(G)\n",
    "                # evaluate state-action-value by taking the average of the state-action returns across all episodes of experience\n",
    "                Q[states[i],actions[i]]= np.mean(returns[states[i]][actions[i]])\n",
    "            \n",
    "        policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "        policy_greedy = greedy_policy(Q, nS, nA)\n",
    "\n",
    "        # save number of policy iterations\n",
    "        num_policy_iter.append(n)\n",
    "        # evaluate average episode reward\n",
    "        average_reward = evaluate_average_reward(env, nA, policy, num_eval_episodes = 100)\n",
    "        \n",
    "        average_rewards_lists.append(average_reward)\n",
    "\n",
    "    return Q, policy, num_policy_iter, average_rewards_lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-policy MC control\n",
    "----------------------\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offpolicy_control(env, nS, nA, num_episodes, gamma, epsilon):\n",
    "\n",
    "    # initialize action-value function\n",
    "    Q = np.zeros([nS,nA])\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "    \n",
    "    # cumulative sum of weights, W, for a given state-action value\n",
    "    C = np.zeros([nS,nA])\n",
    "\n",
    "    # initialize random policy\n",
    "    b_policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "\n",
    "    average_reward = evaluate_average_reward(env, nA, b_policy, num_eval_episodes = 100)\n",
    "    \n",
    "    # store number of policy iterations and average episode reward during training\n",
    "    num_policy_iter = [0]\n",
    "    average_rewards_list_t = [average_reward]\n",
    "    average_rewards_list_b = [average_reward]\n",
    "\n",
    "    # number of policy iterations\n",
    "    n = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        n+=1\n",
    "        # initialize initial state value\n",
    "        state = 0\n",
    "\n",
    "        # save states, actions and rewards for each episode\n",
    "        states = [0]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # loop until episode terminates or truncates \n",
    "        while True:\n",
    "            b_policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "            \n",
    "            # pick action according to the current policy\n",
    "            action = random.choices(A, weights=[b_policy[state][i] for i in range(nA)], k=1)[0]\n",
    "\n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if terminated == False and truncated == False:\n",
    "                # save state, action and reward in episode\n",
    "                states.append(next_state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # update current state\n",
    "                state = next_state\n",
    "\n",
    "            # if epsiode terminates or truncates, reset episode and exit loop\n",
    "            if terminated == True or truncated == True:\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                env.reset()\n",
    "                break\n",
    "    \n",
    "        # initialize return value\n",
    "        G = 0\n",
    "\n",
    "        # initialize importance sampling ratio\n",
    "        W = 1\n",
    "\n",
    "        episode_length = len(states)\n",
    "    \n",
    "        for i in reversed(range(episode_length)):\n",
    "\n",
    "            # return at the current state-action pair is equal to the sum of the immediate reward plus the future discounted return\n",
    "            G = gamma*G + rewards[i]\n",
    "            \n",
    "            # cumulative sum of weights for each state-action pair\n",
    "            C[states[i],actions[i]] = C[states[i],actions[i]] + W\n",
    "\n",
    "            # incremental update rule for Q\n",
    "            Q[states[i],actions[i]]= Q[states[i],actions[i]] + ((W/C[states[i],actions[i]])* (G-Q[states[i],actions[i]]))\n",
    "\n",
    "            t_policy = greedy_policy(Q,nS,nA)\n",
    "\n",
    "            # if action taken is not the optimal action under the target policy\n",
    "            if actions[i] != np.argmax(t_policy[states[i]]):\n",
    "                # then t_policy(s,a) = 0 (as target policy is deterministic) and so update of W = 0\n",
    "                break\n",
    "            \n",
    "            # if action taken is the optimial action, update W\n",
    "            else:\n",
    "                # where the probability the optimial action is taken under target policy is 1\n",
    "                W = W * (1/b_policy[states[i]][actions[i]])\n",
    "\n",
    "        average_reward_t = evaluate_average_reward(env, nA, t_policy, num_eval_episodes = 100)\n",
    "        average_reward_b = evaluate_average_reward(env, nA, b_policy, num_eval_episodes = 100)\n",
    "\n",
    "        average_rewards_list_t.append(average_reward_t)\n",
    "        average_rewards_list_b.append(average_reward_b)\n",
    "\n",
    "        num_policy_iter.append(n)\n",
    "    \n",
    "    return Q, t_policy, b_policy, num_policy_iter, average_rewards_list_t, average_rewards_list_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA\n",
    "-----\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(env, nS, nA, num_episodes, alpha, gamma, epsilon):\n",
    "\n",
    "    # initialize action-value function\n",
    "    Q = np.zeros([nS,nA])\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "\n",
    "    # intialise epsilon-greedy policy\n",
    "    policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "\n",
    "    average_reward = evaluate_average_reward(env, nA, policy, num_eval_episodes = 100)\n",
    "    \n",
    "    # store number of policy iterations and average episode reward during training\n",
    "    num_policy_iter = [0]\n",
    "    average_rewards_list = [average_reward]\n",
    "    \n",
    "    # number of policy iterations\n",
    "    n = 0\n",
    "    \n",
    "    # loop over number of episodes of experience sampled by agent \n",
    "    for _ in range(num_episodes):\n",
    "        n+= 1\n",
    "\n",
    "        # initialize starting state of episode\n",
    "        state = 0 \n",
    "        \n",
    "        # Choose a random action to start\n",
    "        action = env.action_space.sample() \n",
    "\n",
    "        # loop until episode terminates or truncates \n",
    "        while True:\n",
    "            # take chosen action and observe next state and reward\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # use the epsilon-greedy algorithm in order to select an action\n",
    "            next_action = random.choices(A, weights=[policy[next_state][i] for i in range(nA)], k=1)[0]\n",
    "            \n",
    "            #update action-value function\n",
    "            Q[state][action] = Q[state][action] + alpha * (reward + (gamma * Q[next_state][next_action])-Q[state][action])\n",
    "\n",
    "            if terminated == False and truncated == False:\n",
    "                # update current state\n",
    "                state = next_state\n",
    "\n",
    "                # update current action \n",
    "                action = next_action\n",
    "\n",
    "                # update current policy\n",
    "                policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "            \n",
    "            # if epsiode terminates or truncates, reset episode and exit loop\n",
    "            if terminated == True or truncated == True:\n",
    "                # save number of policy iterations\n",
    "                num_policy_iter.append(n)\n",
    "                # evaluate average episode reward\n",
    "                average_reward = evaluate_average_reward(env, nA, policy, num_eval_episodes = 100)\n",
    "                average_rewards_list.append(average_reward)\n",
    "                \n",
    "                env.reset()\n",
    "                break \n",
    "\n",
    "    return Q, policy, num_policy_iter, average_rewards_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "-----------\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, nS, nA, num_episodes, alpha, gamma, epsilon):\n",
    "    \n",
    "    # initialize action-value function\n",
    "    Q = np.zeros([nS,nA])\n",
    "    \n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [a for a in range(nA)]\n",
    "\n",
    "    # behaviour policy is epsilon greedy policy\n",
    "    b_policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "\n",
    "    average_reward = evaluate_average_reward(env, nA, b_policy, num_eval_episodes = 100)\n",
    "    \n",
    "    # store number of policy iterations and average episode reward during training\n",
    "    num_policy_iter = [0]\n",
    "    average_rewards_list_b = [average_reward]\n",
    "    average_rewards_list_t = [average_reward]\n",
    "    \n",
    "    # number of policy iterations\n",
    "    n = 0\n",
    "\n",
    "    # loop over number of episodes of experience sampled by agent \n",
    "    for _ in range(num_episodes):\n",
    "        n += 1\n",
    "        # initialize state value\n",
    "        state = 0\n",
    "\n",
    "        # loop until episode terminates or truncates \n",
    "        while True:\n",
    "            # pick action according to the current behaviour policy\n",
    "            action = random.choices(A, weights=[policy[state][i] for i in range(nA)], k=1)[0]\n",
    "            \n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # update action-value function \n",
    "            Q[state][action] = Q[state][action] = Q[state][action] + alpha * (reward + (gamma * np.max(Q[next_state]))-Q[state][action])\n",
    "\n",
    "            if terminated == False and truncated == False:\n",
    "                # update current state\n",
    "                state = next_state\n",
    "\n",
    "                # update behaviour policy (epsilon-greedy policy)\n",
    "                b_policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "                # update target policy (greedy policy)\n",
    "                t_policy = greedy_policy(Q, nS, nA)\n",
    "            \n",
    "            # if epsiode terminates or truncates, reset episode and exit loop\n",
    "            if terminated == True or truncated == True:\n",
    "\n",
    "                # save number of policy iterations\n",
    "                num_policy_iter.append(n)\n",
    "                # evaluate average episode reward\n",
    "                average_reward_b = evaluate_average_reward(env, nA, b_policy, num_eval_episodes = 100)\n",
    "                average_reward_t = evaluate_average_reward(env, nA, t_policy, num_eval_episodes = 100)\n",
    "                \n",
    "                average_rewards_list_b.append(average_reward_b)\n",
    "                average_rewards_list_t.append(average_reward_t)\n",
    "\n",
    "                env.reset()\n",
    "                break \n",
    "        \n",
    "    return Q, t_policy, b_policy, num_policy_iter, average_rewards_list_b, average_rewards_list_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "div_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
