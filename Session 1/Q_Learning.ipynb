{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equal probability for selecting any action in any state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(nS,nA):\n",
    "    action_prob = [1/nA] * nA\n",
    "    return [action_prob for _ in range(nS)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Always select the action with the highest Q-value (predicted future reward) for the current state to exploit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q, nS, nA):\n",
    "    # initialize arbitrary policy\n",
    "    policy = np.zeros([nS,nA])\n",
    "    \n",
    "    # list of states in environment \n",
    "    S = [i for i in range(nS)]\n",
    "\n",
    "    # loop over states\n",
    "    for state in S:\n",
    "        if all([ q == 0 for q in Q[state]]) :\n",
    "            policy[state] = random_policy(1,nA)[0]\n",
    "        \n",
    "        else:\n",
    "            best_action = np.argmax(Q[state]) \n",
    "            policy[state] = np.eye(nA)[best_action].tolist()\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ε-greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the exploration rate ε\n",
    "- Generate a random number between 0 and 1.\n",
    "- If the generated random number is less than ε, select a random action to explore.\n",
    "- Otherwise, select the action with the highest Q-value for the current state to exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, nS, nA, epsilon):\n",
    "    # initialize arbitraty policy\n",
    "    policy = [[0 for _ in range(nA)] for _ in range (nS)]\n",
    "    \n",
    "    # list of states in environment \n",
    "    S = [i for i in range(nS)]\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "\n",
    "    # loop over states\n",
    "    for state in S:\n",
    "\n",
    "        # if all Q values in state are 0, agent picks random action\n",
    "        if all([ q == 0 for q in Q[state][:]]):\n",
    "            for action in range(nA):\n",
    "                policy[state][action] = 1/nA\n",
    "\n",
    "        else:\n",
    "            # position of maximum Q value in state\n",
    "            maxQ = np.argmax(Q[state])\n",
    "\n",
    "            # probability agent acts randomly\n",
    "            prob = [epsilon/nA for _ in range(nA)]\n",
    "\n",
    "            # probability agent acts greedily\n",
    "            prob[maxQ] = 1 - epsilon  + (epsilon/nA)\n",
    "\n",
    "            for action in A:\n",
    "                policy[state][action] = prob[action]\n",
    "    return policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate policy performance\n",
    "----------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate average episode reward following a given policy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def evaluate_average_reward(env, nA, policy, num_eval_episodes = 100):\n",
    "    # store episode rewards \n",
    "    episode_rewards = []\n",
    "\n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [i for i in range(nA)]\n",
    "    \n",
    "    # loop over number of episodes \n",
    "    for _ in range(num_eval_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "\n",
    "        # loop until episode ends\n",
    "        while True:\n",
    "            # pick action according to the current policy\n",
    "            action = random.choices(A, weights=[policy[state][i] for i in range(nA)], k=1)[0]\n",
    "\n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # update current state\n",
    "            state = next_state\n",
    "\n",
    "            # count cumulative episode reward:\n",
    "            episode_reward += reward\n",
    "        \n",
    "            # exit loop when episode is terminated or truncated\n",
    "            if terminated==True or truncated == True:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                break\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    reward_average = np.mean(episode_rewards)\n",
    "\n",
    "    return reward_average"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen Lake environment from gymnasium API\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\",render_mode=\"ansi\",is_slippery=False)\n",
    "\n",
    "# restart episode with agent in state 0\n",
    "next_state, info = env.reset()\n",
    "\n",
    "print(env.render())\n",
    "\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "print(f\"Size of state space: {nS} \\nSize of action space: {nA}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "------\n",
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning aims to learn an optimal policy by estimating the values of state-action pairs. The Q-value of a state-action pair represents the expected cumulative reward the agent will receive by taking that action in that state and following the optimal policy thereafter.\n",
    "\n",
    "At each time step, the agent selects an action based on the current Q-values, typically using an epsilon-greedy policy that selects the action with the highest Q-value with a probability of 1-epsilon, and selects a random action with a probability of epsilon. After taking an action, the agent observes the next state and the corresponding reward.\n",
    "\n",
    "The Q-Learning algorithm continues to update the Q-values through iterations until the Q-values converge to their optimal values. Once the Q-values are learned, the agent can select actions by choosing the action with the highest Q-value in each state, forming an optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q-Learning (off-policy TD control) algorithm:\")\n",
    "Image(filename='Pseudo-code/off-policy_TD_control.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate [0,1]\n",
    "alpha = # ...\n",
    "# Discount factor [0,1]\n",
    "gamma = # ...\n",
    "# Exploration rate [0,1]\n",
    "epsilon = # ...\n",
    "# Number of episodes [1,infty]\n",
    "num_episodes = # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, nS, nA, num_episodes, alpha, gamma, epsilon):\n",
    "    \n",
    "    # initialize arbitrary action-value function\n",
    "    Q = # ...\n",
    "    \n",
    "    # list of possible actions to be taken by the agent\n",
    "    A = [a for a in range(nA)]\n",
    "\n",
    "    # intialise epsilon-greedy behaviour policy\n",
    "    b_policy = epsilon_greedy_policy(Q, nS, nA, epsilon)\n",
    "\n",
    "    # evaluate initial average episode reward\n",
    "    average_reward = evaluate_average_reward(env, nA, b_policy, num_eval_episodes = 100)\n",
    "    \n",
    "    # store number of policy iterations and average episode reward during training\n",
    "    num_policy_iter = [0]\n",
    "    average_rewards_list_b = [average_reward]\n",
    "    average_rewards_list_t = [average_reward]\n",
    "    \n",
    "    # number of policy iterations\n",
    "    n = 0\n",
    "\n",
    "    # loop over number of episodes of experience sampled by agent \n",
    "    for _ in range(num_episodes):\n",
    "        n += 1\n",
    "        # initialize state value\n",
    "        state = 0\n",
    "\n",
    "        # loop until episode terminates or truncates \n",
    "        while True:\n",
    "            # pick action according to the current behaviour policy\n",
    "            action = #...\n",
    "            \n",
    "            # take action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # update action-value function \n",
    "            Q[state][action] = # ...\n",
    "\n",
    "            if terminated == False and truncated == False:\n",
    "                # update current state\n",
    "                state = next_state\n",
    "\n",
    "                # update behaviour policy (epsilon-greedy policy)\n",
    "                b_policy = # ...\n",
    "                # update target policy (greedy policy)\n",
    "                t_policy = # ...\n",
    "            \n",
    "            # if epsiode terminates or truncates, reset episode and exit loop\n",
    "            if terminated == True or truncated == True:\n",
    "\n",
    "                # save number of policy iterations\n",
    "                num_policy_iter.append(n)\n",
    "                # evaluate average episode reward\n",
    "                average_reward_b = evaluate_average_reward(env, nA, b_policy, num_eval_episodes = 100)\n",
    "                average_reward_t = evaluate_average_reward(env, nA, t_policy, num_eval_episodes = 100)\n",
    "                \n",
    "                average_rewards_list_b.append(average_reward_b)\n",
    "                average_rewards_list_t.append(average_reward_t)\n",
    "\n",
    "                # reset environment\n",
    "                # exit loop\n",
    "        \n",
    "    return Q, t_policy, b_policy, num_policy_iter, average_rewards_list_t, average_rewards_list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, t_policy, b_policy, num_policy_iter, average_rewards_list_t, average_rewards_list_b = Q_learning(env, nS, nA, num_episodes, alpha, gamma, epsilon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the results\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rate\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(num_policy_iter, average_rewards_list_b, color = 'black', label = \"behaviour policy\")\n",
    "ax.plot(num_policy_iter, average_rewards_list_t, color = 'red', linestyle = 'dashed', label = \"target policy\")\n",
    "\n",
    "ax.set_xlabel(\"Number of policy iterations\")\n",
    "ax.set_ylabel('Average reward')\n",
    "\n",
    "ax.legend(prop = { \"size\": 10 })\n",
    "\n",
    "plt.xticks(np.arange(num_policy_iter[0], num_policy_iter[-1]+1,  num_policy_iter[-1]*0.1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
